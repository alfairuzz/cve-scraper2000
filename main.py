import streamlit as st
import streamlit_shadcn_ui as ui
from selenium import webdriver
from bs4 import BeautifulSoup
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from requests_html import HTMLSession
from bs4 import BeautifulSoup
import pandas as pd

from parse_apple_link import get_apple_details
from parse_google_chrome_link import get_google_chrome_details
from parse_oracle_link import get_oracle_details
from parse_sap_link import get_sap_details
from parse_fortinet_link import get_fortiguard_details

import pandas as pd
import os
from io import BytesIO
import time


#st.header("Webscraper UI")
st.markdown("<h1>Webscraper UI <small style='font-size: 12px;'>by maf</small></h1>", unsafe_allow_html=True)
ui.badges(badge_list=[("GOI", "default"), ("Annex A", "secondary")], class_name="felx gap-2", key = "main_badges1")

st.info('Currently avialable for the following products', icon="‚ÑπÔ∏è")

"""
1) Apple (iOS, iPadOS & macOS) - https://support.apple.com/en-sg/100100
2) Google (Stable Channel Update for Desktop) - https://chromereleases.googleblog.com
3) Oracle (Critical Patch Updates) - https://www.oracle.com/sg/security-alerts/
4) SAP (Security Patch Day) - https://support.sap.com/en/my-support/knowledge-base/security-notes-news.html?anchorId=section_370125364
"""

# Side bar
#st.sidebar.header(("‚òï About"))
st.sidebar.markdown("<h1 style='font-size:28px;'>‚òï About</h1>", unsafe_allow_html=True)
st.sidebar.markdown((
    "Webscraper UI is an application designed to extract key vulnerability details, such as CVE information, affected products, and CVE base scores, from specified product websites. This tool simplifies the process of gathering security-related data for analysis."
))

#st.sidebar.header(("üöÄ Usage"))
st.sidebar.markdown("<h1 style='font-size:28px;'>üöÄ Usage</h1>", unsafe_allow_html=True)
st.sidebar.markdown((
    "Simply enter the URL into the search bar, and the application will extract and summarise the relevant information into a downloadable .csv or .xlsx file."
))

st.header("üîç Search", divider=True)


# Streamlit part
url = st.text_input("Enter a Website URL: ")

def convert_df(df):
    # IMPORTANT: Cache the conversion to prevent computation on every rerun
    return df.to_csv().encode("utf-8")

# Function to convert DataFrame to Excel format
def convert_df_to_excel(df):
    output = BytesIO()
    # Use Pandas ExcelWriter to write the DataFrame to Excel in memory
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False)
    # Seek to the beginning of the stream
    output.seek(0)
    return output

# Function to render content with Selenium
def get_rendered_html(url):
    
    if "fortiguard" in url:
        # Set up Selenium WebDriver options
        options = Options()
        options.add_argument("--incognito") # Run Chrome in incognito mode
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option('useAutomationExtension', False)
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
            
        # Start WebDriver and get the URL
        driver = webdriver.Chrome(service=Service(ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()), options=options)
            
        driver.get(url)
        
        # Wait until the specific element is loaded using XPath
        try:
            WebDriverWait(driver, 20).until(
                EC.presence_of_element_located((By.XPATH, '//*[@id="full-page"]/section[3]/div'))
            )
        except Exception as e:
            print("Error: Element not found or page took too long to load.", e)
        
        # Get the page source after JavaScript execution
        html_content = driver.page_source
        
    else:
        
        # Set up Selenium WebDriver options
        options = Options()
        options.add_argument("--incognito") # Run Chrome in incognito mode
        options.add_argument("--headless")  # Run in headless mode for Streamlit
        
        # Start WebDriver and get the URL
        driver = webdriver.Chrome(service=Service(ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()), options=options)
            
        driver.get(url)
        
        # Allow the page to load and render JavaScript
        time.sleep(3)
        
        # Get the page source after JavaScript execution
        html_content = driver.page_source
        
        # Close the WebDriver
        driver.quit()
    
    return html_content

# Scrape the website and update session state
if st.button("Scrape Site"):
    st.write("Scraping the website...")
    
    try:
        # Scrape and render the website content using Selenium
        html_content = get_rendered_html(url)
        # Parse the HTML with BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Extract information using your custom function
        if "support.apple" in url:
            information = get_apple_details(soup)
        elif "chromereleases" in url:
            information = get_google_chrome_details(soup)
        elif 'oracle.com' in url: # returns a list of dataframes
            information = get_oracle_details(soup)
        elif 'support.sap.com' in url:
            information = get_sap_details(soup)
        elif 'www.fortiguard.com' in url:
            information = get_fortiguard_details(soup)

        # Store the scraped data in session state
        st.session_state.dom_content = information

        # Display the information in the text area
        with st.expander("View Content"):
            if 'oracle' in url:
                for i, df in enumerate(information):
                    st.write(f"DataFrame {i+1}")
                    st.dataframe(df.reset_index(drop=True))
            else:
                st.dataframe(information)

    except Exception as e:
        st.error(f"An error occurred: {e}")
        #st.error(f"Product link not supported... yet!")

# Check if "dom_content" exists in session state before showing download button
if "dom_content" in st.session_state and (isinstance(st.session_state.dom_content, pd.DataFrame) or (isinstance(st.session_state.dom_content, list) and st.session_state.dom_content)):
    
    # Convert session content to DataFrame
    information = st.session_state.dom_content
    
    # Convert the DataFrame or list of DataFrames into CSV
    if 'support.apple' in url:
        # information.to_excel(r"./data/Apple Updates/apple_ios_macos.xlsx", index=None)
        csv = convert_df(information)
        # Convert the DataFrame to an Excel file
        excel_data = convert_df_to_excel(information)

        st.download_button(
            label="Download data as CSV",
            data=csv,
            file_name="apple_security_updates.csv",
            mime="text/csv",
        )

        # Use st.download_button to allow Excel download
        st.download_button(
            label="Download data as Excel",
            data=excel_data,
            file_name="apple_security_updates.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
        # Display a message after the file is downloaded
        st.balloons()
        st.write("File is ready for download!")
        
    elif 'chromereleases' in url:
        # information.to_excel(r'./data/Google Updates/stable-channel-update-for-desktop.xlsx', index=None)
        csv = convert_df(information)
        # Convert the DataFrame to an Excel file
        excel_data = convert_df_to_excel(information)

        st.download_button(
            label="Download data as CSV",
            data=csv,
            file_name="stable_channel_update_for_desktop.csv",
            mime="text/csv",
        )

        # Use st.download_button to allow Excel download
        st.download_button(
            label="Download data as Excel",
            data=excel_data,
            file_name="stable_channel_update_for_desktop.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
        # Display a message after the file is downloaded
        st.balloons()
        st.write("File is ready for download!")
        
    elif 'oracle.com' in url:
        # Combine all DataFrames in the list into a single DataFrame
        combined_df = pd.concat(information, ignore_index=True)
        csv = convert_df(combined_df)
        # Convert the DataFrame to an Excel file
        excel_data = convert_df_to_excel(combined_df)

        st.download_button(
            label="Download data as CSV",
            data=csv,
            file_name="oracle_security_tables.csv",
            mime="text/csv",
        )

        # Use st.download_button to allow Excel download
        st.download_button(
            label="Download data as Excel",
            data=excel_data,
            file_name="oracle_security_tables.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
        
        # Display a message after the file is downloaded
        st.balloons()
        st.write("File is ready for download!")
        
    elif 'support.sap.com' in url:
        # information.to_excel(r'./data/SAP Updates/sap_security_table.xlsx', index=None)
        csv = convert_df(information)
        # Convert the DataFrame to an Excel file
        excel_data = convert_df_to_excel(information)

        st.download_button(
            label="Download data as CSV",
            data=csv,
            file_name="sap_security_table.csv",
            mime="text/csv",
        )

        # Use st.download_button to allow Excel download
        st.download_button(
            label="Download data as Excel",
            data=excel_data,
            file_name="sap_security_table.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )

        # Display a message after the file is downloaded
        st.balloons()
        st.write("File is ready for download!")
        
    elif 'www.fortiguard.com' in url:
        # I want to show the table
        # then select which to get further details
        # run get_fortiguard_details function for each check list
        # compile and return dataframe for donwload
        pass
    
else:
    st.write("Enter a Website URL to begin scraping for information.")
