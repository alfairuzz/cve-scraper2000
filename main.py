import streamlit as st
from selenium import webdriver
from bs4 import BeautifulSoup
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType
from requests_html import HTMLSession
from bs4 import BeautifulSoup
import pandas as pd

from parse_apple_link import get_apple_details
from parse_google_chrome_link import get_google_chrome_details
from parse_oracle_link import get_oracle_details
from parse_sap_link import get_sap_details

import pandas as pd
import os
import time


# Streamlit part
url = st.text_input("Enter a Website URL: ")

def convert_df(df):
    # IMPORTANT: Cache the conversion to prevent computation on every rerun
    return df.to_csv().encode("utf-8")

# Function to render content with Selenium
def get_rendered_html(url):
    
    # Set up Selenium WebDriver options
    options = Options()
    options.add_argument("--incognito") # Run Chrome in incognito mode
    options.add_argument("--headless")  # Run in headless mode for Streamlit
    
    # Start WebDriver and get the URL
    driver = webdriver.Chrome(service=Service(ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()), options=options)
        
    driver.get(url)
    
    # Allow the page to load and render JavaScript
    time.sleep(3)
    
    # Get the page source after JavaScript execution
    html_content = driver.page_source
    
    # Close the WebDriver
    driver.quit()
    
    return html_content

# Scrape the website and update session state
if st.button("Scrape Site"):
    st.write("Scraping the website...")
    
    try:
        # Scrape and render the website content using Selenium
        html_content = get_rendered_html(url)

        # Parse the HTML with BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Extract information using your custom function
        if "support.apple" in url:
            information = get_apple_details(soup)
        elif "chromereleases" in url:
            information = get_google_chrome_details(soup)
        elif 'oracle.com' in url: # returns a list of dataframes
            information = get_oracle_details(soup)
        elif 'support.sap.com' in url:
            information = get_sap_details(soup)

        # Store the scraped data in session state
        st.session_state.dom_content = information

        # Display the information in the text area
        with st.expander("View Content"):
            if 'oracle' in url:
                for i, df in enumerate(information):
                    st.write(f"DataFrame {i+1}")
                    st.dataframe(df.reset_index(drop=True))
            else:
                st.dataframe(information)

    except Exception as e:
        st.error(f"An error occurred: {e}")

# Check if "dom_content" exists in session state before showing download button
if "dom_content" in st.session_state and (isinstance(st.session_state.dom_content, pd.DataFrame) or (isinstance(st.session_state.dom_content, list) and st.session_state.dom_content)):
    
    if st.button("Download File"):
        # Convert session content to DataFrame and save it
        information = st.session_state.dom_content
        
        if 'support.apple' in url:
            #information.to_excel(r"./data/Apple Updates/apple_ios_macos.xlsx", index=None)
            csv = convert_df(information)

            st.download_button(
                label="Download data as CSV",
                data=csv,
                file_name="apple.csv",
                mime="text/csv",
            )
            
            # Display a message after the file is saved
            st.write("File has been saved successfully!")
            
        elif 'chromereleases' in url:
            #information.to_excel(r'./data/Google Updates/stable-channel-update-for-desktop.xlsx', index = None)
            csv = convert_df(information)

            st.download_button(
                label="Download data as CSV",
                data=csv,
                file_name="stable-channel-update-for-desktop.csv",
                mime="text/csv",
            )
            
            # Display a message after the file is saved
            st.write("File has been saved successfully!")
            
        elif 'oracle.com' in url:
            # Specify the file path where you want to save the Excel file
            # Combine all DataFrames in the list into a single DataFrame
            combined_df = pd.concat(information, ignore_index=True)

            csv = convert_df(combined_df)

            st.download_button(
                label="Download data as CSV",
                data=csv,
                file_name="oracle_security_tables.csv",
                mime="text/csv",
            )
            
            # Display a message after the file is saved
            st.write("File has been saved successfully!")
        
        elif 'support.sap.com' in url:
            #information.to_excel(r'./data/SAP Updates/sap_security_table.xlsx', index = None) 
            csv = convert_df(information)

            st.download_button(
                label="Download data as CSV",
                data=csv,
                file_name="sap_security_table.csv",
                mime="text/csv",
            )
            
            # Display a message after the file is saved
            st.write("File has been saved successfully!")
    
else:
    st.write("Enter a Website URL to begin scraping for information.")
