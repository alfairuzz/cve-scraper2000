import os
import time
import logging
from io import BytesIO
import pandas as pd
import qrcode
import streamlit as st
import streamlit_shadcn_ui as ui
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType

# Import asynchronous libraries for faster web scraping
import asyncio
import aiohttp
from parse_apple_link import get_apple_details
from parse_google_chrome_link import get_google_chrome_details
from parse_oracle_link import get_oracle_details
from parse_sap_link import get_sap_details
from parse_fortinet_link import get_fortinet_details, get_complete_fortinet_details

# Set up logging
logging.basicConfig(level=logging.INFO)

# ------------------- Streamlit UI Setup -------------------

st.markdown(
    "<h1>Webscraper UI <small style='font-size: 12px;'>by maf</small></h1>",
    unsafe_allow_html=True
)

ui.badges(
    badge_list=[("GOI", "default"), ("Annex A", "secondary")],
    class_name="flex gap-2",
    key="main_badges1"
)

# Informational messages
st.info('Currently available for the following products', icon="‚ÑπÔ∏è")

# Define product URLs
product_urls = {
    "Apple (iOS, iPadOS & macOS)": "https://support.apple.com/en-sg/100100",
    "Google (Stable Channel Update)": "https://chromereleases.googleblog.com",
    "Oracle (Critical Patch Updates)": "https://www.oracle.com/sg/security-alerts/",
    "SAP (Security Patch Day)": "https://support.sap.com/en/my-support/knowledge-base/security-notes-news.html?anchorId=section_370125364",
    "Fortiguard (PSIRT Advisory)": "https://www.fortiguard.com/psirt?product=FortiClientMac"
}

# Output URLs in a formatted list
for product, url in product_urls.items():
    st.markdown(f"- **{product}** - [Link Here]({url})")

# Sidebar Information
st.sidebar.markdown("<h1 style='font-size:28px;'>‚òï About</h1>", unsafe_allow_html=True)
st.sidebar.markdown(
    "Webscraper UI is an application designed to extract key vulnerability details, "
    "such as CVE information, affected products, and CVE base scores, from specified "
    "product websites."
)

st.sidebar.markdown("<h1 style='font-size:28px;'>üöÄ Usage</h1>", unsafe_allow_html=True)
st.sidebar.markdown(
    "Simply enter the URL into the search bar, and the application will extract and "
    "summarize the relevant information into a downloadable .xlsx file."
)

st.header("üîç Search", divider=True)

# ------------------- Input Field -------------------
url = st.text_input("Enter a Website URL:")

# ------------------- Helper Functions -------------------

@st.cache_data
def convert_df_to_excel(df):
    output = BytesIO()
    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
        df.to_excel(writer, index=False)
    output.seek(0)
    return output

async def async_fetch(url):
    """
    Asynchronous function to fetch the HTML content of a given URL.
    
    Parameters:
    url (str): The URL to fetch.

    Returns:
    str: The HTML content of the fetched page.
    """
    async with aiohttp.ClientSession() as session:
        async with session.get(url) as response:
            return await response.text()

async def scrape_content(url):
    """
    Scrapes the content of a URL asynchronously.

    Parameters:
    url (str): The URL to scrape.

    Returns:
    BeautifulSoup object: Parsed HTML content of the page.
    """
    html_content = await async_fetch(url)
    return BeautifulSoup(html_content, 'html.parser')

def provide_download_buttons(information, url):
    if 'support.apple' in url:
        excel_data = convert_df_to_excel(information)
        st.download_button(
            label="Download data as Excel",
            data=excel_data,
            file_name="apple_security_updates.xlsx",
            mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        )
        st.success("File is ready for download!")

    # Similar logic for other products...

# ------------------- Main Logic -------------------
if st.button("Scrape Site"):
    st.write("Scraping the website...")
    try:
        # Use async for faster web scraping
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        soup = loop.run_until_complete(scrape_content(url))

        if "support.apple" in url:
            information = get_apple_details(soup)
        # Additional URL handlers...
        
        if information is not None:
            st.session_state.dom_content = information
            with st.expander("View Content"):
                st.dataframe(information.reset_index(drop=True))
            st.success("Scraping completed successfully!")
        else:
            st.error("Failed to extract information from the URL.")

    except Exception as e:
        st.error(f"An error occurred: {e}")
        logging.exception("Error during scraping.")

# Check if content exists before showing download options
if "dom_content" in st.session_state and st.session_state.dom_content is not None:
    provide_download_buttons(st.session_state.dom_content, url)
else:
    st.write("Enter a Website URL to begin scraping for information.")
