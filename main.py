import streamlit as st
import streamlit_shadcn_ui as ui
from selenium import webdriver
from bs4 import BeautifulSoup
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType
from requests_html import HTMLSession
from bs4 import BeautifulSoup
import pandas as pd

from parse_apple_link import get_apple_details
from parse_google_chrome_link import get_google_chrome_details
from parse_oracle_link import get_oracle_details
from parse_sap_link import get_sap_details

import pandas as pd
import os
import time


st.header("Webscraper UI")
ui.badges(badge_list=[("GOI", "default"), ("Annex A", "secondary")], class_name="felx gap-2", key = "main_badges1")

st.info('Currently avialable for the following products', icon="‚ÑπÔ∏è")

"""
1) Apple (iOS, iPadOS & macOS) - https://support.apple.com/en-sg/100100
2) Google (Stable Channel Update for Desktop) - https://chromereleases.googleblog.com
3) Oracle (Critical Patch Updates) - https://www.oracle.com/sg/security-alerts/
4) SAP (Security Patch Day) - https://support.sap.com/en/my-support/knowledge-base/security-notes-news.html?anchorId=section_370125364
"""

# Side bar
#st.sidebar.header(("‚òï About"))
st.sidebar.markdown("<h1 style='font-size:28px;'>‚òï About</h1>", unsafe_allow_html=True)
st.sidebar.markdown((
    "Webscraper UI is an application designed to extract key vulnerability details, such as CVE information, affected products, and CVE base scores, from specified product websites. This tool simplifies the process of gathering security-related data for analysis."
))

st.sidebar.header(("üöÄ Usage"))
st.sidebar.markdown((
    "Simply enter the URL into the search bar, and the application will extract and summarise the relevant information into a downloadable .csv file."
))

st.header("üîç Search", divider=True)

# # Initialize session state for the dynamic text inputs
# if "input_count" not in st.session_state:
#     st.session_state.input_count = 1  # Start with one input

# # Function to add a new text input
# def add_input():
#     st.session_state.input_count += 1

# # Function to remove the last text input (keeping a minimum of 1)
# def remove_input():
#     if st.session_state.input_count > 1:
#         st.session_state.input_count -= 1

# # Display the dynamic text inputs and store them into the session state
# url_list = []
# for i in range(st.session_state.input_count):
#     url = st.text_input(f"Enter URL {i+1}", key=f"url_input_{i+1}")
#     if url:  # Only append non-empty URLs
#         url_list.append(url)

# # Add and Remove buttons
# st.button("Add", on_click=add_input)
# st.button("Remove", on_click=remove_input)

# # Display the collected URL list
# st.write("Collected URLs:", url_list)

# Streamlit part
url = st.text_input("Enter a Website URL: ")

def convert_df(df):
    # IMPORTANT: Cache the conversion to prevent computation on every rerun
    return df.to_csv().encode("utf-8")

# Function to render content with Selenium
def get_rendered_html(url):
    
    # Set up Selenium WebDriver options
    options = Options()
    options.add_argument("--incognito") # Run Chrome in incognito mode
    options.add_argument("--headless")  # Run in headless mode for Streamlit
    
    # Start WebDriver and get the URL
    driver = webdriver.Chrome(service=Service(ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()), options=options)
        
    driver.get(url)
    
    # Allow the page to load and render JavaScript
    time.sleep(3)
    
    # Get the page source after JavaScript execution
    html_content = driver.page_source
    
    # Close the WebDriver
    driver.quit()
    
    return html_content

# Scrape the website and update session state
if st.button("Scrape Site"):
    st.write("Scraping the website...")
    
    try:
        # Scrape and render the website content using Selenium
        html_content = get_rendered_html(url)

        # Parse the HTML with BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Extract information using your custom function
        if "support.apple" in url:
            information = get_apple_details(soup)
        elif "chromereleases" in url:
            information = get_google_chrome_details(soup)
        elif 'oracle.com' in url: # returns a list of dataframes
            information = get_oracle_details(soup)
        elif 'support.sap.com' in url:
            information = get_sap_details(soup)

        # Store the scraped data in session state
        st.session_state.dom_content = information

        # Display the information in the text area
        with st.expander("View Content"):
            if 'oracle' in url:
                for i, df in enumerate(information):
                    st.write(f"DataFrame {i+1}")
                    st.dataframe(df.reset_index(drop=True))
            else:
                st.dataframe(information)

    except Exception as e:
        st.error(f"An error occurred: {e}")

# Check if "dom_content" exists in session state before showing download button
if "dom_content" in st.session_state and (isinstance(st.session_state.dom_content, pd.DataFrame) or (isinstance(st.session_state.dom_content, list) and st.session_state.dom_content)):
    
    # Convert session content to DataFrame
    information = st.session_state.dom_content
    
    # Convert the DataFrame or list of DataFrames into CSV
    if 'support.apple' in url:
        # information.to_excel(r"./data/Apple Updates/apple_ios_macos.xlsx", index=None)
        csv = convert_df(information)

        st.download_button(
            label="Download data as CSV",
            data=csv,
            file_name="apple.csv",
            mime="text/csv",
        )
        
    elif 'chromereleases' in url:
        # information.to_excel(r'./data/Google Updates/stable-channel-update-for-desktop.xlsx', index=None)
        csv = convert_df(information)

        st.download_button(
            label="Download data as CSV",
            data=csv,
            file_name="stable-channel-update-for-desktop.csv",
            mime="text/csv",
        )
        
    elif 'oracle.com' in url:
        # Combine all DataFrames in the list into a single DataFrame
        combined_df = pd.concat(information, ignore_index=True)

        csv = convert_df(combined_df)

        st.download_button(
            label="Download data as CSV",
            data=csv,
            file_name="oracle_security_tables.csv",
            mime="text/csv",
        )
        
    elif 'support.sap.com' in url:
        # information.to_excel(r'./data/SAP Updates/sap_security_table.xlsx', index=None)
        csv = convert_df(information)

        st.download_button(
            label="Download data as CSV",
            data=csv,
            file_name="sap_security_table.csv",
            mime="text/csv",
        )

    # Display a message after the file is downloaded
    st.balloons()
    st.write("File is ready for download!")
    
else:
    st.write("Enter a Website URL to begin scraping for information.")
