import streamlit as st
from selenium import webdriver
from bs4 import BeautifulSoup
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType
from requests_html import HTMLSession
from bs4 import BeautifulSoup
import pandas as pd

import pandas as pd
import os
import time

# Streamlit part
url = st.text_input("Enter a Website URL: ")

def get_apple_details(soup):
    print('getting apple data')
    
    # Find all paragraphs with the class 'gb-paragraph'
    paragraphs = soup.find_all('p', class_='gb-paragraph')

    # Keywords for relevant information
    keywords = ['Available for', 'Impact', 'Description', 'CVE']

    # Phrases to exclude from the results
    exclude_phrases = [
        'This document describes', 
        'For our customers\' protection', 
        'Apple security documents', 
        'For more information about security', 
        'Released'
    ]

    # Initialize lists for storing information
    available_for_list = []
    impact_list = []
    description_list = []
    cve_list = []

    # Temporary variables to store data
    available_for = None
    impact = None
    description = None
    cve = None

    # Filter and populate the lists with relevant data
    for paragraph in paragraphs:
        text = paragraph.get_text().strip()
        
        # Check if the paragraph contains any of the relevant keywords and exclude unwanted content
        if any(keyword in text for keyword in keywords) and not any(exclude_phrase in text for exclude_phrase in exclude_phrases):
            
            # Identify and store the relevant information
            if 'Available for' in text:
                available_for = text.split('Available for: ')[-1]
            elif 'Impact' in text:
                impact = text.split('Impact: ')[-1]
            elif 'Description' in text:
                description = text.split('Description: ')[-1]
            elif 'CVE' in text:
                cve = text.split(':')[0]
                
                # Once we reach the CVE number, append the row data to the lists
                available_for_list.append(available_for)
                impact_list.append(impact)
                description_list.append(description)
                cve_list.append(cve)

    # Create a DataFrame
    df = pd.DataFrame({
        'Available for': available_for_list,
        'Impact': impact_list,
        'Description': description_list,
        'CVE': cve_list
    })

    return df

def convert_df(df):
    # IMPORTANT: Cache the conversion to prevent computation on every rerun
    return df.to_csv().encode("utf-8")

# Function to render content with Selenium
def get_rendered_html(url):
    
    # Set up Selenium WebDriver options
    options = Options()
    options.add_argument("--incognito") # Run Chrome in incognito mode
    options.add_argument("--headless")  # Run in headless mode for Streamlit
    
    # Start WebDriver and get the URL
    driver = webdriver.Chrome(service=Service(ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install()), options=options)
        
    driver.get(url)
    
    # Allow the page to load and render JavaScript
    time.sleep(3)
    
    # Get the page source after JavaScript execution
    html_content = driver.page_source
    
    # Close the WebDriver
    driver.quit()
    
    return html_content

# Scrape the website and update session state
if st.button("Scrape Site"):
    st.write("Scraping the website...")
    
    try:
        # Scrape and render the website content using Selenium
        html_content = get_rendered_html(url)

        # Parse the HTML with BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Extract information using your custom function
        if "support.apple" in url:
            information = get_apple_details(soup)

        # Store the scraped data in session state
        st.session_state.dom_content = information

        # Display the information in the text area
        with st.expander("View Content"):
            if 'oracle' in url:
                for i, df in enumerate(information):
                    st.write(f"DataFrame {i+1}")
                    st.dataframe(df.reset_index(drop=True))
            else:
                st.dataframe(information)

    except Exception as e:
        st.error(f"An error occurred: {e}")

# Check if "dom_content" exists in session state before showing download button
if "dom_content" in st.session_state and (isinstance(st.session_state.dom_content, pd.DataFrame) or (isinstance(st.session_state.dom_content, list) and st.session_state.dom_content)):
    
    if st.button("Download File"):
        # Convert session content to DataFrame and save it
        information = st.session_state.dom_content
        
        if 'support.apple' in url:
            #information.to_excel(r"./data/Apple Updates/apple_ios_macos.xlsx", index=None)
            csv = convert_df(information)

            st.download_button(
                label="Download data as CSV",
                data=csv,
                file_name="apple.csv",
                mime="text/csv",
            )
            
            
        # Display a message after the file is saved
        st.write("File has been saved successfully!")
else:
    st.write("Enter a Website URL to begin scraping for information.")
