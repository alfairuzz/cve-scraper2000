from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import os
import re


# Function to break up the CVE column into ID, Impact, and CVE
def extract_info(row):
    # Regex pattern to match the CVE (if any)
    match = re.search(r"(CVE-\d{4}-\d+)", row)
    if match:
        cve = match.group(1)
        # Remove the CVE from the row
        row = row.replace(cve, "").strip()
    else:
        cve = ""

    # Splitting the ID from the Impact using the first space
    parts = row.split(" ", 1)
    if len(parts) == 2:
        id_ = parts[0].strip()
        impact = parts[1].strip()
    else:
        id_ = parts[0].strip()
        impact = ""

    return pd.Series([id_, impact, cve])


def get_fortiguard_details(soup):

    # For example, extract div data:
    div_content = soup.find("div", {"id": "full-page"})

    # Extract the specific section using the id and tag structure
    section_content = soup.select_one("#full-page section:nth-of-type(3)")

    if section_content:
        # Now, grab the div with class "container-xxl" inside this section
        container_div = section_content.find("div", class_="container-xxl")
        
        # Create a list to store the extracted data
        data = []
        
        if container_div:
            # Loop through all divs with class "row" inside the container
            rows = container_div.find_all("div", class_="row")
            
            for row in rows:
                # Extract the required fields from each row
                cve_text = row.find("div", class_="col-md-3").get_text(strip=True)
                date = row.find("div", class_="col d-none d-lg-block").get_text(strip=True)

                # Break the cve_text into ID, Impact, and CVE using a regex-based approach
                match = re.search(r"(CVE-\d{4}-\d+)", cve_text)
                if match:
                    cve = match.group(1)
                    # Remove the CVE from the text to separate ID and Impact
                    cve_text = cve_text.replace(cve, "").strip()
                else:
                    cve = ""
                
                # Splitting the remaining text into ID and Impact
                parts = cve_text.split(" ", 1)
                if len(parts) == 2:
                    id_ = parts[0].strip()
                    impact = parts[1].strip()
                    url_link = f"https://www.fortiguard.com/psirt/{id_}"
                else:
                    id_ = parts[0].strip()
                    impact = ""
                    url_link = ""

                
                # Append the extracted data as a dictionary to the data list
                data.append({
                    'ID': id_,
                    'Impact': impact,
                    'CVE': cve,
                    'Date': date,
                    "Link": url_link,
                })
            
            # Convert the list of dictionaries into a pandas DataFrame
            df = pd.DataFrame(data)
            
            # Print or save the DataFrame
            return(df)
        else:
            print("Div with class 'container-xxl' not found")
    else:
        print("Section not found")

# Idea is:
            # """
            # 1) Scrape the main page for what to be updated. (done)
            # 2) get the initial dataframe table and output in streamlit the dates available to select (or maybe using data_editor)
            # 3) Then it will return the data editor df and we can use the true/false value to determine which website to scrape
            # 4) Using this detail, grab the url link and scrape the page.
            # 5) will need to get the link to the CVSS base score page too and grab that information.
            
            # """