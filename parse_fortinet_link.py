import re
import pandas as pd
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from webdriver_manager.core.os_manager import ChromeType
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import os
import time


def get_rendered_html(url):
    """
    Uses Selenium to render JavaScript content and retrieve the HTML source.

    Parameters:
    url (str): The URL to retrieve.

    Returns:
    str: The rendered HTML content.
    """
    # Set up Selenium WebDriver options
    options = Options()
    options.add_argument("--incognito")
    options.add_argument("--headless")  # Run in headless mode for Streamlit
    options.add_argument(
        "user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 "
        "Safari/537.36"
    )

    # Start WebDriver and get the URL
    service = Service(ChromeDriverManager(chrome_type=ChromeType.CHROMIUM).install())
    driver = webdriver.Chrome(service=service, options=options)
    driver.get(url)

    # Allow the page to load and render JavaScript
    time.sleep(10)

    # Get the page source after JavaScript execution
    html_content = driver.page_source

    # Close the WebDriver
    driver.quit()

    return html_content


def get_fortinet_details(soup):
    """
    Extracts vulnerability details from the Fortinet PSIRT page.

    Parameters:
    soup (BeautifulSoup): The parsed HTML content of the Fortinet PSIRT page.

    Returns:
    pd.DataFrame: A DataFrame containing 'ID', 'Impact', 'CVE', 'Date', and 'Link' details.
    """
    # Attempt to select the specific section containing the data
    section_content = soup.select_one("#full-page section:nth-of-type(3)")

    if not section_content:
        print("Section not found")
        return pd.DataFrame()  # Return empty DataFrame if section not found

    # Find the container with the required data
    container_div = section_content.find("div", class_="container-xxl")

    if not container_div:
        print("Div with class 'container-xxl' not found")
        return pd.DataFrame()  # Return empty DataFrame if container not found

    # Initialize a list to store the extracted data
    data_list = []

    # Loop through all divs with class "row" inside the container
    rows = container_div.find_all("div", class_="row")

    for row in rows:
        # Extract the required fields from each row
        cve_text_div = row.find("div", class_="col-md-3")
        date_div = row.find("div", class_="col d-none d-lg-block")

        # Skip if essential data is missing
        if not cve_text_div or not date_div:
            continue

        cve_text = cve_text_div.get_text(strip=True)
        date = date_div.get_text(strip=True)

        # Extract CVE using regex
        match = re.search(r"(CVE-\d{4}-\d+)", cve_text)
        if match:
            cve = match.group(1)
            # Remove the CVE from the text to separate ID and Impact
            cve_text = cve_text.replace(cve, "").strip()
        else:
            cve = ""

        # Split the remaining text into ID and Impact
        parts = cve_text.split(" ", 1)
        id_ = parts[0].strip() if parts else ''
        impact = parts[1].strip() if len(parts) > 1 else ''

        # Construct the URL link
        url_link = f"https://www.fortiguard.com/psirt/{id_}" if id_ else ''

        # Append the extracted data as a dictionary to the data list
        data_list.append({
            'ID': id_,
            'Impact': impact,
            'CVE': cve,
            'Date': date,
            'Link': url_link,
        })

    # Convert the list of dictionaries into a pandas DataFrame
    df = pd.DataFrame(data_list)

    return df

def get_div_content_df(div_content):
    # Initialize an empty list to store the data
    data = []
    for row in div_content.find_all('tr'):
        cols = row.find_all('td')
        if len(cols) == 2:
            key = cols[0].get_text(strip=True)
            # Check if there's a link in the value
            link_tag = cols[1].find('a')
            if link_tag:
                value = link_tag.get_text(strip=True)
                link = link_tag.get('href')
            else:
                value = cols[1].get_text(strip=True)
                link = None
            data.append((key, value, link))

    # Convert the list of tuples into a dictionary
    result = {}
    for key, value, link in data:
        result[key] = value
        if link:
            # Store the link in a separate key with 'Link' appended
            result[f"{key} Link"] = link

    # Create a DataFrame with one row
    df = pd.DataFrame([result])

    # Define the desired column order
    columns_order = [
        'IR Number', 'Date', 'Component', 'Severity',
        'CVSSv3 Score', 'CVSSv3 Score Link',
        'Impact', 'CVE ID', 'CVE ID Link',
        'CVRF', 'CVRF Link'
    ]

    # Handle missing columns
    for col in columns_order:
        if col not in df.columns:
            df[col] = None

    # Reorder the DataFrame columns
    df = df[columns_order]
    cvss_url_link = df['CVSSv3 Score Link'][0]
    df['CVSS 3.1 Base Score'] = get_true_base_score(cvss_url_link)
    
    return df

def get_div_content_two_df(div_content):
    # Extract headers
    headers = []
    for th in div_content.find('thead').find_all('th'):
        headers.append(th.get_text(strip=True))

    # Extract rows
    rows = []
    for tr in div_content.find('tbody').find_all('tr'):
        cols = tr.find_all('td')
        cols = [td.get_text(strip=True) for td in cols]
        rows.append(cols)

    # Create DataFrame
    df = pd.DataFrame(rows, columns=headers)
    
    # Only keep the affected versions
    df_affected = df[df['Affected'].str.lower() != "not affected"]
    
    return df_affected

def get_true_base_score(url_link):
    html_content = get_rendered_html(url_link) 

    # Parse the content with BeautifulSoup
    soup = BeautifulSoup(html_content, 'html.parser')

    # Corrected the usage of 'class' in the find method
    cvss_base_score = soup.find("dd", {"id": "cvss-base-score-cell"}).text
    
    return cvss_base_score

def get_complete_fortinet_details(url_list):
    # Initialize lists to store DataFrames
    df_list1 = []
    df_list2 = []

    # Loop through each URL
    for link in url_list:
        print(f"Processing {link}...")
        # Fetch the HTML content
        html_content = get_rendered_html(link)
        
        # Parse the content with BeautifulSoup
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Corrected the usage of 'class' in the find method
        div_content = soup.find("table", {"class": "table table-responsive table-borderless meta"})
        div_content_two = soup.find("table", {"class": "table table-borderless table-striped table-dark"})
        
        cve_id = ''
        
        # Check if tables are found
        if div_content is not None:
            df1 = get_div_content_df(div_content)
            # Add a column for the URL
            df1['URL'] = link
            cve_id = df1['CVE ID'][0]
            df_list1.append(df1)
            
        else:
            print(f"Table 1 not found in {link}")
        
        if div_content_two is not None:
            df2 = get_div_content_two_df(div_content_two)
            # Add a column for the CVE
            df2['CVE ID'] = cve_id
            df2.insert(0, 'CVE ID', df2.pop('CVE ID'))
            df_list2.append(df2)
        else:
            print(f"Table 2 not found in {link}")

    # Concatenate the DataFrames
    if df_list1:
        df_combined1 = pd.concat(df_list1, ignore_index=True)
    else:
        df_combined1 = pd.DataFrame()
        print("No data collected for the first DataFrame.")

    if df_list2:
        df_combined2 = pd.concat(df_list2, ignore_index=True)
    else:
        df_combined2 = pd.DataFrame()
        print("No data collected for the second DataFrame.")

    return df_combined1, df_combined2
