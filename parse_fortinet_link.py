import re
import pandas as pd
from bs4 import BeautifulSoup

def get_fortinet_details(soup):
    """
    Extracts vulnerability details from the Fortinet PSIRT page.

    Parameters:
    soup (BeautifulSoup): The parsed HTML content of the Fortinet PSIRT page.

    Returns:
    pd.DataFrame: A DataFrame containing 'ID', 'Impact', 'CVE', 'Date', and 'Link' details.
    """
    # Attempt to select the specific section containing the data
    section_content = soup.select_one("#full-page section:nth-of-type(3)")

    if not section_content:
        print("Section not found")
        return pd.DataFrame()  # Return empty DataFrame if section not found

    # Find the container with the required data
    container_div = section_content.find("div", class_="container-xxl")

    if not container_div:
        print("Div with class 'container-xxl' not found")
        return pd.DataFrame()  # Return empty DataFrame if container not found

    # Initialize a list to store the extracted data
    data_list = []

    # Loop through all divs with class "row" inside the container
    rows = container_div.find_all("div", class_="row")

    for row in rows:
        # Extract the required fields from each row
        cve_text_div = row.find("div", class_="col-md-3")
        date_div = row.find("div", class_="col d-none d-lg-block")

        # Skip if essential data is missing
        if not cve_text_div or not date_div:
            continue

        cve_text = cve_text_div.get_text(strip=True)
        date = date_div.get_text(strip=True)

        # Extract CVE using regex
        match = re.search(r"(CVE-\d{4}-\d+)", cve_text)
        if match:
            cve = match.group(1)
            # Remove the CVE from the text to separate ID and Impact
            cve_text = cve_text.replace(cve, "").strip()
        else:
            cve = ""

        # Split the remaining text into ID and Impact
        parts = cve_text.split(" ", 1)
        id_ = parts[0].strip() if parts else ''
        impact = parts[1].strip() if len(parts) > 1 else ''

        # Construct the URL link
        url_link = f"https://www.fortiguard.com/psirt/{id_}" if id_ else ''

        # Append the extracted data as a dictionary to the data list
        data_list.append({
            'ID': id_,
            'Impact': impact,
            'CVE': cve,
            'Date': date,
            'Link': url_link,
        })

    # Convert the list of dictionaries into a pandas DataFrame
    df = pd.DataFrame(data_list)

    return df
